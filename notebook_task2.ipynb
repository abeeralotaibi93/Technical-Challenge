{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install transformers medmnist pillow torchvision huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from medmnist import PneumoniaMNIST\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace with your actual HuggingFace token\n",
    "# login(\"your_token_here\")\n",
    "# Or use: login()  # will prompt interactively\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data directory if it doesn't exist\n",
    "data_root = './data'\n",
    "os.makedirs(data_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_dataset = PneumoniaMNIST(root=data_root, split='train', download=True)\n",
    "val_dataset   = PneumoniaMNIST(root=data_root, split='val',   download=True)\n",
    "test_dataset  = PneumoniaMNIST(root=data_root, split='test',  download=True)\n",
    "\n",
    "print(f\"Training samples:   {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples:       {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MedGemma model and processor\n",
    "model_id = \"google/medgemma-4b-it\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    \"\"\"\n",
    "    Convert PneumoniaMNIST image to RGB 224x224 PIL image.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    if not isinstance(img, Image.Image):\n",
    "        arr = img.squeeze()\n",
    "        if hasattr(arr, 'numpy'):\n",
    "            arr = arr.numpy()\n",
    "        arr = np.array(arr, dtype=np.uint8)\n",
    "        img = Image.fromarray(arr)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = transforms.Resize((224, 224))(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample image\n",
    "sample_index = 0\n",
    "img, label = train_dataset[sample_index]\n",
    "img = preprocess_image(img)\n",
    "\n",
    "ground_truth = \"Pneumonia\" if label == 1 else \"Normal\"\n",
    "print(f\"Ground Truth: {ground_truth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "prompt_text = (\n",
    "    \"You are an expert radiologist. \"\n",
    "    \"Generate a chest X-ray report following RSNA pneumonia guidelines. \"\n",
    "    \"Include only findings consistent with the standard criteria \"\n",
    "    \"(consolidation, effusion, pneumothorax, lung opacity, heart size, \"\n",
    "    \"mediastinum, pleura, bones) and provide a structured summary with \"\n",
    "    \"Findings and Impression.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the chat messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are an expert radiologist.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\",  \"text\":  prompt_text},\n",
    "            {\"type\": \"image\", \"image\": img}\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "print(f\"Input token length: {input_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prediction\n",
    "print(\"Generating prediction... (CPU may take 1-3 minutes)\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "generated_ids = output_ids[0][input_len:]\n",
    "prediction = processor.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "print(\"Generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and results\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Ground Truth: {ground_truth}\", fontsize=13)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PREDICTION:\")\n",
    "print(\"=\"*60)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optional: Batch evaluation on multiple test samples ---\n",
    "\n",
    "def run_inference(img_pil, prompt_text, model, processor, device, max_new_tokens=300):\n",
    "    \"\"\"Run inference on a single PIL image.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": [{\"type\": \"text\", \"text\": \"You are an expert radiologist.\"}]},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": [\n",
    "             {\"type\": \"text\",  \"text\":  prompt_text},\n",
    "             {\"type\": \"image\", \"image\": img_pil}\n",
    "         ]}\n",
    "    ]\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    generated_ids = output_ids[0][input_len:]\n",
    "    return processor.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "# Evaluate on first 3 test samples\n",
    "num_samples = 3\n",
    "print(f\"Running inference on {num_samples} test samples...\\n\")\n",
    "\n",
    "for i in range(num_samples):\n",
    "    raw_img, lbl = test_dataset[i]\n",
    "    pil_img = preprocess_image(raw_img)\n",
    "    gt = \"Pneumonia\" if lbl == 1 else \"Normal\"\n",
    "    pred = run_inference(pil_img, prompt_text, model, processor, device, max_new_tokens=200)\n",
    "\n",
    "    print(f\"--- Sample {i} | Ground Truth: {gt} ---\")\n",
    "    print(pred)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
