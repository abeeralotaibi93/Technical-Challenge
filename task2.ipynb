# -*- coding: utf-8 -*-
"""Task_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1khWMm5cdlwZZo6k-AwsLetxdCrhEN1qb
"""

import os
import torch
from transformers import AutoProcessor, AutoModelForImageTextToText
from PIL import Image
from torchvision import transforms
from medmnist import PneumoniaMNIST
import matplotlib.pyplot as plt

from huggingface_hub import login
login(os.environ["HF_TOKEN"])

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Create the data directory if it doesn't exist
data_root = './data'
os.makedirs(data_root, exist_ok=True)

# Now load the datasets
train_dataset = PneumoniaMNIST(root=data_root, split='train', download=True)
val_dataset   = PneumoniaMNIST(root=data_root, split='val', download=True)
test_dataset  = PneumoniaMNIST(root=data_root, split='test', download=True)

print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Test samples: {len(test_dataset)}")

model_id = "google/medgemma-4b-it"

processor = AutoProcessor.from_pretrained(model_id)
model = AutoModelForImageTextToText.from_pretrained(model_id)

def preprocess_image(img):
    """
    Convert PneumoniaMNIST image to RGB 224x224 PIL image
    """
    if not isinstance(img, Image.Image):
        img = Image.fromarray(img.squeeze().numpy())
    img = img.convert("RGB")
    transform = transforms.Resize((224,224))
    img = transform(img)
    return img

sample_index = 0
img, label = train_dataset[sample_index]
img = preprocess_image(img)

ground_truth = "Pneumonia" if label == 1 else "Normal"

prompt_text = (
    "You are an expert radiologist. "
    "Generate a chest X-ray report following RSNA pneumonia guidelines. Include only findings consistent with the standard criteria (consolidation, effusion, pneumothorax, lung opacity, heart size, mediastinum, pleura, bones) and provide a structured summary with Findings and Impression."
)

messages = [
    {"role": "system", "content": [{"type": "text", "text": "You are an expert radiologist."}]},
    {"role": "user", "content": [
        {"type": "text", "text": prompt_text},
        {"type": "image", "image": img}
    ]}
]

# Prepare Inputs
# ------------------------------
inputs = processor.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt"
).to(device)

input_len = inputs["input_ids"].shape[-1]

# ------------------------------
print("Generating prediction... (CPU may take 1-3 minutes)")

with torch.inference_mode():
    output_ids = model.generate(
        **inputs,
        max_new_tokens=150,
        do_sample=False
    )

generated_ids = output_ids[0][input_len:]
prediction = processor.decode(generated_ids, skip_special_tokens=True).strip()

# Display Results
# ------------------------------
plt.figure(figsize=(3,3))
plt.imshow(img)
plt.title(f"Ground Truth: {ground_truth}")
plt.axis("off")
plt.show()

print("Model Prediction:", prediction)
